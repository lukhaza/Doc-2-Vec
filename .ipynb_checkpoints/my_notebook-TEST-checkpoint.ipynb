{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing python data processing and Natural-language processing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "import nltk\n",
    "import string\n",
    "import multiprocessing\n",
    "import time\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.models.doc2vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert gensim.models.doc2vec.FAST_VERSION > -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punc(post):\n",
    "    '''function for removing punctuation from post'''\n",
    "    punc_num = string.punctuation + '0123456789'\n",
    "    return ''.join([l for l in post if l not in punc_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    '''function for removing stopwords from the list tokens'''\n",
    "    sss = set(stopwords.words('english'))\n",
    "    return [t for t in tokens if t not in sss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mbti = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_mbti = []\n",
    "for i,r in mbti.iterrows():\n",
    "    for comment in r['posts'].split('|||'):\n",
    "        all_mbti.append([r['type'],comment])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "all_mbti['post'] = all_mbti['post'].str.lower()\n",
    "all_mbti['post'] = all_mbti['post'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokeniser = TreebankWordTokenizer()\n",
    "all_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>post</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp and intj moments  urlweb  sportscenter no...</td>\n",
       "      <td>[enfp, and, intj, moments, urlweb, sportscente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>what has been the most lifechanging experience...</td>\n",
       "      <td>[what, has, been, the, most, lifechanging, exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>urlweb   urlweb  on repeat for most of today</td>\n",
       "      <td>[urlweb, urlweb, on, repeat, for, most, of, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>may the perc experience immerse you</td>\n",
       "      <td>[may, the, perc, experience, immerse, you]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>the last thing my infj friend posted on his fa...</td>\n",
       "      <td>[the, last, thing, my, infj, friend, posted, o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               post  \\\n",
       "2  INFJ  enfp and intj moments  urlweb  sportscenter no...   \n",
       "3  INFJ  what has been the most lifechanging experience...   \n",
       "4  INFJ       urlweb   urlweb  on repeat for most of today   \n",
       "5  INFJ                may the perc experience immerse you   \n",
       "6  INFJ  the last thing my infj friend posted on his fa...   \n",
       "\n",
       "                                              tokens  \n",
       "2  [enfp, and, intj, moments, urlweb, sportscente...  \n",
       "3  [what, has, been, the, most, lifechanging, exp...  \n",
       "4  [urlweb, urlweb, on, repeat, for, most, of, to...  \n",
       "5         [may, the, perc, experience, immerse, you]  \n",
       "6  [the, last, thing, my, infj, friend, posted, o...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mbti = all_mbti[all_mbti['post']!='urlweb']\n",
    "all_mbti.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(all_mbti['tokens'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Doc2Vec model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting the model parameters and initialising the model. The parameters were mostly found by trail and error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "vec_size = 50\n",
    "alpha = 0.040\n",
    "\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha,\n",
    "                window = 4 ,\n",
    "                min_alpha=0.033,\n",
    "                min_count=3,\n",
    "                dm =1,\n",
    "                max_vocab_size=20000,\n",
    "                workers=cores,\n",
    "                negative = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary before training the model otherwise training will fail\n",
    "model.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  training loop\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "                # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "model.save(\"first.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "model= Doc2Vec.load(\"first.model\")\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('kids', 0.7476826310157776),\n",
       " ('families', 0.6871461868286133),\n",
       " ('students', 0.6676808595657349),\n",
       " ('tension', 0.6578578948974609),\n",
       " ('legs', 0.6477729678153992),\n",
       " ('customers', 0.6355262994766235),\n",
       " ('colours', 0.6316623687744141),\n",
       " ('answers', 0.6242372989654541),\n",
       " ('girls', 0.6158963441848755),\n",
       " ('friends', 0.6112593412399292)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('children')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems well trained. Most of the top 10 words make logical sense as to why they world be similar to the word 'children'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words used in very different contexts should have very low scores for similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1291517880655949"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1='dirty',w2='clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words used in similar contexts should have high similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6842354163303089"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1='husband',w2='wife')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
